\documentclass[10pt]{article} 

\usepackage{fullpage}
\usepackage{bookmark}

\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{physics}
% \usepackage{unicode-math}
\usepackage{dsfont}

\usepackage[shortlabels]{enumitem}
\usepackage[noabbrev, nameinlink]{cleveref}
\usepackage[most]{tcolorbox}
\usepackage{empheq}
\usepackage[amsmath,standard,thmmarks]{ntheorem} 
\usepackage{bm}
\usepackage{tabularray}
\usepackage{pdfpages}
\usepackage{float}

\setlist[enumerate]{topsep=1pt,itemsep=0pt,partopsep=1ex,parsep=1ex}

% floor, ceiling, set
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\iprod}{\langle}{\rangle}
\DeclarePairedDelimiter{\card}{\lvert}{\rvert}
\let\abs\relax
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\let\norm\relax
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\bdy}{bdy}
\DeclareMathOperator{\Lim}{Lim}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\dual}{dual}
\DeclareMathOperator{\opt}{opt}
\DeclareMathOperator{\cone}{cone}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\NB}{NB}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\indeg}{indeg}
\DeclareMathOperator{\outdeg}{outdeg}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\odd}{odd}
\DeclareMathOperator{\OPT}{OPT}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\ri}{ri}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Prox}{Prox}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\zer}{zer}
\DeclareMathOperator{\fl}{fl}
\DeclareMathOperator{\mach}{mach}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Softmax}{Softmax}
\DeclareMathOperator{\Po}{Po}
\DeclareMathOperator{\Be}{Be}
\DeclareMathOperator{\Cov}{Cov}

\newcommand{\code}[1]{\lstinline{#1}}

\newcommand{\ones}{\mathds{1}}

\newcommand{\up}{\uparrow}
\newcommand{\down}{\downarrow}
\newcommand{\tends}[1]{\xrightarrow{#1}}
\newcommand{\eq}[1]{\stackrel{#1}{=}}
\newcommand{\Geq}[1]{\stackrel{#1}{\geq}}
\newcommand{\Leq}[1]{\stackrel{#1}{\leq}}

% commonly used sets
\newcommand{\m}{\mathds{m}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\P}{\mathbb{P}}

\newcommand{\B}{\mathcal{B}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\renewcommand{\S}{\mathcal{S}}

\newcommand{\W}{\mathbf{W}}
\newcommand{\w}{\mathbf{w}}
\renewcommand{\c}{\mathbf{c}}
\renewcommand{\d}{\mathbf{d}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\f}{\mathbf{f}}

\newcommand{\h}{\vec{h}}
\newcommand{\p}{\vec{p}}
\renewcommand{\a}{\vec{a}}
\renewcommand{\b}{\vec{b}}
\renewcommand{\t}{\vec{t}}
\renewcommand{\u}{\vec{u}}
\renewcommand{\v}[1]{\vec{#1}}

\newcommand{\sset}{\subseteq}
\newcommand{\mcal}{\mathcal}
\newcommand{\mscr}{\mathscr}
\newcommand{\mbf}{\mathbf}
\newcommand{\mat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\eff}{\text{eff}}

\newcommand{\NP}{\ensuremath{\mathcal{NP}}}

\newtcbtheorem[
  number within=section,
  crefname={lemma}{Lemma}
]
{lem}
{Lemma}%
{
  theorem style=break,
  sharp corners=all,
  colframe=Red,
  colback={White!95!Red},
  coltitle=black,
  fonttitle=\bfseries,
  beforeafter skip=12pt
}{lem}

\newtcbtheorem[no counter]{pf}{Proof}%
{
  breakable,
  blanker,
  left=5.5mm,
  borderline west={2pt}{0pt}{NavyBlue!80!white},
  after upper=\null\nobreak\hfill\ensuremath{\square},
  colback=white,
  colframe=white,
  coltitle=black,
  fonttitle=\it,
  parbox=false,
  after skip=12pt
}{pf}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{
    frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\setlength\parindent{0pt}
\setlength{\parskip}{8pt}

\setcounter{secnumdepth}{2}
\renewcommand\thesection{Problem \arabic{section}.}
\renewcommand\thesubsection{(\arabic{subsection})}


\begin{document}

\begin{center}
    {\Large\textbf{Yale University}}\\
    \vspace{3mm}
    {\Large\textbf{S\&DS 551, Spring 2023}}\\
    \vspace{2mm}
    {\Large\textbf{Homework 2}}\\
    \vspace{3mm}
    \textbf{Chang Feng (Felix) Zhou cz397}
\end{center}

\section{}
First,
we note that the random walk $S$ can only return to 0
at an even time step.

We wish to count the number of binary strings of length $2n$
that is \emph{balanced} (equal number of 0s and 1s)
subject to the condition that any prefix is not balanced.
This corresponds to a random walk which returns to 0 at time $2n$
but not before that.

By symmetry,
it suffices to double the number of binary strings
on $n$ 1s and $n-1$ 0s such that any prefix contains strictly more 1s than 0s
(the $0$-th character is prepended as 0 to ensure balance).
We claim that there are
\[
  \frac1{2n-1} \binom{2n-1}{n-1}
\]
such strings.

Before we prove the claim.
Note that if the claim holds,
then we are done.
This is because the desired probability is then given by
\begin{align*}
  \frac{\frac1{2n-1} \binom{2n-1}{n-1} \cdot 2}{2^{2n}}
  &= \frac1{2n-1} \binom{2n}{n} 2^{-2n}
\end{align*}
as required.

To see the claim,
we observe that there is a bijection between 
the number of binary strings
on $n$ 1s and $n-1$ 0s such that any prefix contains strictly more 1s than 0s
and the number of binary strings
on $n$ 1s and $n$ 0s such that any prefix contains at least as many 1s as 0s.
Indeed,
the bijection is obtained by prepending a 0 to the $(2n-1)$-bit string.
But the cardinality of the latter set
is precisely given by the well-known $(n-1)$-th Catalan number
\[
  C_n
  = \frac1{2n-1} \binom{2n-1}{n-1}
  = \frac1{2n-1} \binom{2n-1}{n}.
\]

Finally,
we have
\begin{align*}
  \E[T^\alpha]
  &= \sum_{n=1}^\infty \P\set{T=2n}\cdot (2n)^\alpha \\
  &= \sum_{n=1}^\infty \frac{(2n)^\alpha}{2n-1} \binom{2n}{n} 2^{-2n} \\
  &= \sum_{n=1}^\infty \frac{(2n)^\alpha}{2n-1} \frac{(2n)!}{n!n!} 2^{-2n} \\
  &\approx \sum_{n=1}^\infty \frac1{2n-1} \frac{(2n)^{2n+\frac12+\alpha} e^{-2n} \sqrt{2\pi}}{\left( n^{n+\frac12} e^{-n} \sqrt{2\pi} \right)^2} 2^{-2n} \\
  &= \frac1{\sqrt{2\pi}} \sum_{n=1}^\infty \frac1{2n-1} \frac{(2n)^{2n+\frac12+\alpha}}{n^{2n+1}} 2^{-2n} \\
  &= \frac{2^{\frac12+\alpha}}{\sqrt{2\pi}} \sum_{n=1}^\infty \frac1{2n-1} n^{\alpha-\frac12}.
\end{align*}

For $\alpha < \frac12$,
this is series is bounded above by a convergent $p$-series
\[
  \sum_{n\geq 1} \frac1{n^p}
\]
for some $p > 1$.
On the other hand,
for $\alpha \geq \frac12$,
this series is bounded below by the divergent harmonic series
\[
  \sum_{n\geq 1} \frac1n.
\]

This concludes the proof.

\clearpage
\section{}
We have that
\begin{align*}
  \P\set{R_{n} = R_{n-1} + 1}
  &= \P\set{S_n\neq S_{n-1}, \dots, S_n\neq S_0} \\
  &= \P\set{S_1\neq 0, \dots, S_n\neq 0} \\
  &= \P\set{S_1S_2\dots S_n\neq 0}.
\end{align*}
\iffalse
Note that here we used the fact
\[
  \sum_{i=1}^k X_i \eq{d} \sum_{i=1}^k X_{n-i+1}
\]
since the $X_i$'s are iid.
\fi

Now let $x_n := \E[R_n]$ and $p_n := \P\set{R_n = R_{n-1} + 1}$.
By the downward continuity of measure,
we have that as $n\to \infty$,
\[
  p_n\to \ell =: \P\set{\forall k\geq 1, S_k=0}.
\]

By the law of total expectation,
\begin{align*}
  x_n
  &= \E[R_n\mid R_n=R_{n-1}]\cdot (1-p_n) + \E[R_n\mid R_n=R_{n-1}+1]\cdot p_n \\
  &= x_{n-1} (1-p) + (x_{n-1} + 1) p_n \\
  &= x_{n-1} + p_n \\
  &= \sum_{k=1}^n p_k.
\end{align*}

Recall that if a real-valued sequence converges,
then the average of the partial sums also converge to the same limit.
It follows that
\[
  \frac1n \E[R_n]
  = \frac1n \sum_{k=1}^n p_k
  \to \ell
\]
as $n\to \infty$.

Finally,
we wish to show that in the case of simple random walks,
\[
  \ell = \abs{p-q}.
\]
Without loss of generality,
let us assume that $p\geq q$
and show that $\ell = p-q$.

Equivalently,
we can show that the probability of eventually hitting 0 is $2q$.
Then $\ell = 1-2q = p-q$.
Let $p_k$ denote the probability of eventually hitting 0
starting at position $k > 0$.
Then we have the recurrence
\begin{align*}
  p_1
  &= q + p p_2 \\
  &= q + p p_1^2.
\end{align*}
To see this remark that the increments of the random walk are independent,
thus to arrive at 0 starting at 2
is equivalent to two independent random walks hitting 0 when starting at 1.

Solving this quadratic equation yields solutions $p_1 = 1, \frac qp$.
In the case of $p\geq q$,
we take $\frac qp$
and in the case of $p < q$,
we take 1.
The probability of eventually returning to 0
assuming that $p\geq q$ is thus
\begin{align*}
  &\P\set{\text{return}\mid X_1=-1}\P\set{X_1=-1}
  + \P\set{\text{return}\mid X_1=1}\P\set{X_1=1} \\
  &= q\cdot 1 + p\cdot \frac qp \\
  &= 2q.
\end{align*}
as desired.
To see this,
we condition on the first step being to the left or the right
and apply the appropriate solution to the quadratic equation above.
Note that the scenario of stepping to to the left is equivalent to first stepping to the right
but having $p'=q, q'=p$.

\clearpage
\section{}
Let us rewrite this as the expected hitting time of 0 starting at position $b$
by flipping the probabilities $q'=p, p'=q$.
Our strategy to compute this value is to compute the expected hitting time by setting an artificial boundary at $n$
and then letting $n\to \infty$,
similar to our work in class.
This strategy is justified by the monontone convergence theorem.

From our work in class,
we know that the expected hitting time for $p'=q'=\frac12$ is infinity.
But then the expected hitting time for $p'>q'$ can only be greater
and is thus not finite either.
We focus on the case where $p'\leq q'$.

Let $T_{b, n}$ denote the hitting time of one of two barriers $0, n$
starting at $b\in [0, n]$.
We have
\begin{align*}
  \E[T_{b, n}]
  &= q'\E[T_{b-1, n}] + p'\E[T_{b+1, n}] + 1 \\
  \E[T_{0, n}] &= 0 \\
  \E[T_{n, n}] &= 0.
\end{align*}

Solving this recurrence under the boundary conditions yield
\begin{align*}
  \E[T_{b, n}]
  &= \frac{b}{q'-p'} - \frac{n}{q'-p'}\cdot \frac{\left( \frac{q'}{p'} \right)^b - 1}{\left( \frac{q'}{p'} \right)^n - 1} \\
  &\to \frac{b}{q'-p'}. &&n\to \infty
\end{align*}

Keeping in mind that we flipped the probabilities,
we conclude that the desired expectation is
\[
  \frac{b}{p-q}
\]
as desired.

\clearpage
\section{}
Write $X_n$ to denote the size of the population at time $n$.
Then
\[
  \P\set{T=n} = \P\set{X_n=0} - \P\set{X_{n-1}=0}
\]
with the base case $\P\set{T=0} = 0$.

Let $\eta_n := \P\set{X_n=0}$.
From our work in class,
\begin{align*}
  \eta_0 &= 0 \\
  \eta_{n+1}
  &= \sum_{k=0}^\infty \eta_n^k f(k) \\
  &= \sum_{k=0}^\infty \eta_n^k\cdot qp^k \\
  &= \frac{q}{1-\eta_n p}.
\end{align*}

Define $r := p/q$.
We argue by induction that
\begin{align*}
  \eta_n =
  \begin{cases}
    \frac{n}{n+1}, &p=q=\frac12 \\
    \frac{r^n-1}{r^{n+1}-1}, &p\neq q
  \end{cases}
\end{align*}

\underline{Case I: $p=q=\frac12$}
We see that the formula is correct for the base case of $n=0$.
Suppose inductively that the formula holds up to some $n\in \N$.
Then
\begin{align*}
  \eta_{n+1}
  &= \frac{q}{1-\eta_n p} \\
  &= \frac{1}{1-\eta_n} \\
  &= \frac{n+1}{2(n+1) - n} \\
  &= \frac{n+1}{n+2}
\end{align*}
as required.

In this case for $n\geq 1$,
\begin{align*}
  \P\set{T=n}
  &= \frac{n}{n+1} - \frac{n-1}{n} \\
  &= \frac{n^2 - (n^2-1)}{n(n+1)} \\
  &= \boxed{\frac{1}{n(n+1)}}. 
\end{align*}

Now,
\begin{align*}
  \E[T]
  &= \sum_{n\geq 1} n\cdot \frac1{n(n+1)} \\
  &= \sum_{n\geq 1} \frac1{n+1}
\end{align*}
is the diverging harmonic series so the expectation is infinite.

\underline{Case II: $p\neq q$}
We see that the formula is correct for the base case of $n=0$.
Suppose inductively that the formula holds up to some $n\in \N$.
Then
\begin{align*}
  \eta_{n+1}
  &= \frac{q}{1-\eta_n p} \\
  &= \frac{q}{1- \frac{r^n-1}{r^{n+1}-1} p} \\
  &= \frac{r^{n+1}-1}{\frac1q (r^{n+1}-1) - (r^n-1)r} \\
  &= \frac{r^{n+1}-1}{r^{n+1} (1/q-1) + (r-1/q)} \\
  &= \frac{r^{n+1}-1}{r^{n+2}-1}
\end{align*}
as desired.

In this case for $n\geq 1$,
\begin{align*}
  \P\set{T=n}
  &= \frac{r^n-1}{r^{n+1}-1} - \frac{r^{n-1}-1}{r^{n}-1} \\
  &= \boxed{\frac{r^{n-1} (r-1)^2}{(r^{n+1}-1)(r^n-1)}}.
\end{align*}

Recall for non-negative discrete random variables
we have the identity
\begin{align*}
  \E[T]
  &= \sum_{n\geq 1} n\cdot \P\set{T=n} \\
  &= \sum_{n\geq 1} \P\set{T\geq n} \\
  &= \sum_{n\geq 1} 1- \P\set{T<n} \\
  &= \sum_{n\geq 1} 1- \P\set{T\leq n-1} \\
  &= \sum_{n\geq 0} 1- \P\set{T\leq n} \\
  &= \sum_{n\geq 0} 1- \P\set{X_n = 0} \\
  &= \sum_{n\geq 0} 1 - \frac{r^{n} - 1}{r^{n+1}-1} \\
  &= \sum_{n\geq 0} \frac{r^{n+1} - 1 - r^{n} + 1}{r^{n+1}-1} \\
  &= \sum_{n\geq 0} \frac{r^n(r - 1)}{r^{n+1}-1}.
\end{align*}

For $r < 1$,
this series converges by the ratio test
\begin{align*}
  \frac{r^{n+1}(r-1)}{r^{n+2}-1}\cdot \frac{r^{n+1}-1}{r^n (r-1)}
  &= \frac{r^{n+2} - r}{r^{n+1}-1} \\
  &\to r \\
  &< 1.
\end{align*}

For $r > 1$,
the series diverges as it is element-wise bounded below by a divergent series
\[
  \frac{r^n (r-1)}{r^{n+1} - 1}
  \geq \frac{r^n (r-1)}{r^{n+1}}
  = \frac{r-1}{r}
  \not\to 0.
\]

\clearpage
\section{}
We use the probability generating function
\[
  \Psi_t(x)
  := \E\left[ x^{G_t} \right]
\]
as the main tool to tackle this problem.

First,
let us show that
\[
  \Psi_{t+1}(x) = \Psi_1(\Psi_t(x)).
\]
Indeed,
consider the more general scenario that $Z = \sum_{k=1}^N Y_k$
where $Y_k, N$ are non-negative discrete variables
such that the $Y_k$'s are iid.
We claim that the generating function $\Psi_Z$ of $Z$ satisfies
\[
  \Psi_Z(x) = \Psi_N(\Psi_Y(x)).
\]
First recall that the generating function of independent random variables is the product
\[
  \Psi_{\sum_{k=1}^m Y_k}(x)
  = \E\left[ x^{\sum_{k=1}^m Y_k} \right]
  = \prod_{k=1}^m \E\left[ x^{Y_k} \right].
\]
Observe here that independence is crucial so that we can factorize the expected values.
We can now compute
\begin{align*}
  \Psi_Y(x)
  &= \sum_{k\geq 0} x^k \P\set{Y = k} \\
  &= \sum_{k\geq 0} x^k \sum_{m\geq 0} \P\set{Y = k\mid N=m}\P\set{N=m} \\
  &= \sum_{m\geq 0} \P\set{N=m}\cdot \sum_{k\geq 0} x^k \P\set{Y = k\mid N=m} \\
  &= \sum_{m\geq 0} \P\set{N=m}\cdot \Psi_{\sum_{k=1}^m Y_k}(x) \\
  &= \sum_{m\geq 0} \P\set{N=m}\cdot \Psi_Y(x)^k \\
  &= \Psi_N(\Psi_Y(x)).
\end{align*}

Recall that we can write
\[
  X_{t+1} = \sum_{k=1}^{X_t} X_{t+1, k}
\]
where $X_{t+1, k}\sim \Po(2)$ iid.
It follows that
\begin{align*}
  \Psi_{t+1}(x)
  &= \Psi_t(\Psi_1(x)) \\
  &\dots \\
  &= (\Psi_1\circ \dots\circ \Psi_1) (x) \\
  &= \Psi_1(\Psi_t(x)).
\end{align*}

Next,
we remark that
\[
  \P\set{G_t=1} = \Psi_t'(0).
\]
To see this observe that
\begin{align*}
  \Psi_t'(0)
  &= \sum_{k\geq 1} k(0)^{k-1} \P\set{G_t=k} \\
  &= \P\set{G_t=1}.
\end{align*}

By the chain rule,
\begin{align*}
  \Psi_{t+1}'(x)
  &= \frac{d}{dx} \Psi_1(\Psi_t(x)) \\
  &= \Psi_1'(\Psi_t(x))\cdot \Psi_t'(x) \\
  \Psi_{t+1}'(0)
  &= \Psi_1'(\Psi_t(0))\cdot \Psi_t'(0).
\end{align*}
It follows that
\begin{align*}
  \rho_t
  &:= \frac{\P\set{G_{t+1} = 1}}{\P\set{G_t=1}} \\
  &= \Psi_1'(\Psi_t(0)) \\
  &= \Psi_1'(\eta_t) &&\eta_t := \P\set{G_t = 0} \\
  &= \sum_{k\geq 1} k\eta_t^{k-1} \P\set{G_1 = k} \\
  &= 2 \sum_{k\geq 1} \frac{(2\eta_t)^{(k-1)} e^{-2}}{(k-1)!} \\
  &= 2 \exp(2\eta_t-2) \\
  &\to 2\exp(2\eta - 2).
\end{align*}

Finally,
from our work in class,
the ultimate extinction probability satisfies
\[
  \eta = \Psi_1(\eta) = \exp(2\eta-2).
\]
Hence
\[
  \boxed{\lim_{t\to \infty} \rho_t = 2\eta}.
\]

\end{document}
