\documentclass[10pt]{article} 

\usepackage{fullpage}
\usepackage{bookmark}

\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{mathrsfs}
\usepackage{physics}
% \usepackage{unicode-math}
\usepackage{dsfont}

\usepackage[shortlabels]{enumitem}
\usepackage[noabbrev, nameinlink]{cleveref}
\usepackage[most]{tcolorbox}
\usepackage{empheq}
\usepackage[amsmath,standard,thmmarks]{ntheorem} 
\usepackage{bm}
\usepackage{tabularray}
\usepackage{pdfpages}
\usepackage{float}

\setlist[enumerate]{topsep=1pt,itemsep=0pt,partopsep=1ex,parsep=1ex}

% floor, ceiling, set
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\iprod}{\langle}{\rangle}
\DeclarePairedDelimiter{\card}{\lvert}{\rvert}
\let\abs\relax
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\let\norm\relax
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\DeclareMathOperator{\Int}{int}
\DeclareMathOperator{\bdy}{bdy}
\DeclareMathOperator{\Lim}{Lim}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\col}{col}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\dual}{dual}
\DeclareMathOperator{\opt}{opt}
\DeclareMathOperator{\cone}{cone}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\poly}{poly}
\DeclareMathOperator{\NB}{NB}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\indeg}{indeg}
\DeclareMathOperator{\outdeg}{outdeg}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\odd}{odd}
\DeclareMathOperator{\OPT}{OPT}
\DeclareMathOperator{\aff}{aff}
\DeclareMathOperator{\ri}{ri}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\epi}{epi}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Fix}{Fix}
\DeclareMathOperator{\Prox}{Prox}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\sign}{sign}
\DeclareMathOperator{\zer}{zer}
\DeclareMathOperator{\fl}{fl}
\DeclareMathOperator{\mach}{mach}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\Softmax}{Softmax}
\DeclareMathOperator{\Po}{Po}
\DeclareMathOperator{\Be}{Be}
\DeclareMathOperator{\Cov}{Cov}

\newcommand{\code}[1]{\lstinline{#1}}

\newcommand{\ones}{\mathds{1}}

\newcommand{\up}{\uparrow}
\newcommand{\down}{\downarrow}
\newcommand{\tends}[1]{\xrightarrow{#1}}
\newcommand{\eq}[1]{\stackrel{#1}{=}}
\newcommand{\Geq}[1]{\stackrel{#1}{\geq}}
\newcommand{\Leq}[1]{\stackrel{#1}{\leq}}

% commonly used sets
\newcommand{\m}{\mathds{m}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\renewcommand{\P}{\mathbb{P}}

\newcommand{\B}{\mathcal{B}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\J}{\mathcal{J}}
\renewcommand{\S}{\mathcal{S}}

\newcommand{\W}{\mathbf{W}}
\newcommand{\w}{\mathbf{w}}
\renewcommand{\c}{\mathbf{c}}
\renewcommand{\d}{\mathbf{d}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\f}{\mathbf{f}}

\newcommand{\h}{\vec{h}}
\newcommand{\p}{\vec{p}}
\renewcommand{\a}{\vec{a}}
\renewcommand{\b}{\vec{b}}
\renewcommand{\t}{\vec{t}}
\renewcommand{\u}{\vec{u}}
\renewcommand{\v}[1]{\vec{#1}}

\newcommand{\sset}{\subseteq}
\newcommand{\mcal}{\mathcal}
\newcommand{\mscr}{\mathscr}
\newcommand{\mbf}{\mathbf}
\newcommand{\mat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\eff}{\text{eff}}

\newcommand{\NP}{\ensuremath{\mathcal{NP}}}

\newtcbtheorem[
  number within=section,
  crefname={lemma}{Lemma}
]
{lem}
{Lemma}%
{
  theorem style=break,
  sharp corners=all,
  colframe=Red,
  colback={White!95!Red},
  coltitle=black,
  fonttitle=\bfseries,
  beforeafter skip=12pt
}{lem}

\newtcbtheorem[no counter]{pf}{Proof}%
{
  breakable,
  blanker,
  left=5.5mm,
  borderline west={2pt}{0pt}{NavyBlue!80!white},
  after upper=\null\nobreak\hfill\ensuremath{\square},
  colback=white,
  colframe=white,
  coltitle=black,
  fonttitle=\it,
  parbox=false,
  after skip=12pt
}{pf}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{
    frame=tb,
    language=Python,
    aboveskip=3mm,
    belowskip=3mm,
    showstringspaces=false,
    columns=flexible,
    basicstyle={\small\ttfamily},
    numbers=none,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{dkgreen},
    stringstyle=\color{mauve},
    breaklines=true,
    breakatwhitespace=true,
    tabsize=3
}

\setlength\parindent{0pt}
\setlength{\parskip}{8pt}

\setcounter{secnumdepth}{2}
\renewcommand\thesection{Problem \arabic{section}.}
\renewcommand\thesubsection{(\arabic{subsection})}


\begin{document}

\begin{center}
    {\Large\textbf{Yale University}}\\
    \vspace{3mm}
    {\Large\textbf{S\&DS 551, Spring 2023}}\\
    \vspace{2mm}
    {\Large\textbf{Homework 1}}\\
    \vspace{3mm}
    \textbf{Chang Feng (Felix) Zhou cz397}
\end{center}

\section{}
\subsection{Mean}
Let $X_1, \dots, X_{n-k}$ be random variables denoting the eval of each of the $n-k$ remaining balls
and $X = \sum_{i=1}^{n-k} X_i$ be their sum.
By the linearity of expectation,
\begin{align*}
  \E[X]
  &= \sum_{i=1}^{n-k} \E[X_i] \\
  &= \boxed{(n-k) \frac{n+1}2}.
\end{align*}

\subsection{Variance}
We compute the second moment in similar fashion.
Indeed,
\begin{align*}
  \E[X^2]
  &= \sum_{i, j=1}^{n-k} \E[X_iX_j] \\
  &= \sum_{\ell=1}^{n-k} \E[X_\ell^2] + 2 \sum_{1\leq i<j\leq n-k} \E[X_iX_j].
\end{align*}

For any $\ell\in [n-k]$,
\begin{align*}
  \E[X_\ell^2]
  &= \frac1n \sum_{i=1}^n i^2 \\
  &= \frac{(n+1)(2n+1)}6 \\
  \sum_{\ell=1}^{n-k} \E[X_\ell^2]
  &= \frac{(n-k)(n+1)(2n+1)}6.
\end{align*}

Now,
there are $\binom{n}{2}$ possible pairs $1\leq i< j\leq n$
and $\binom{n-k}2$ pairs of indices from $[n-k]$.
It follows that
\begin{align*}
  2 \sum_{1\leq i< j\leq n-k} \E[X_i X_j]
  &= \frac{\binom{n-k}2}{\binom{n}2} \sum_{1\leq i, j\leq n: i\neq j} ij \\
  &= \frac{(n-k)(n-k-1)}{n(n-1)} \left( \sum_{1\leq i, j\leq n} ij - \sum_{\ell=1}^n \ell^2  \right) \\
  &= \frac{(n-k)(n-k-1)}{n(n-1)} \left( \left[ \sum_{i=1} i \right]^2 - \sum_{j=1}^n j^2  \right) \\
  &= \frac{(n-k)(n-k-1)}{n(n-1)} \left( \frac{n^2(n+1)^2}4 - \frac{n(n+1)(2n+1)}6  \right) \\
  &= \frac{(n-k)(n-k-1)}{n(n-1)} \left( \frac{n(n+1)[3n(n+1) - 2(2n+1)]}{12}  \right) \\
  &= \frac{(n-k)(n-k-1)}{n(n-1)} \left( \frac{n(n+1)(3n^2-n-2)}{12} \right).
\end{align*}

Finally,
\begin{align*}
  \Var[X]
  &= \E[X^2] - \E[X]^2 \\
  &= \frac{(n-k)(n+1)(2n+1)}6
  + \frac{(n-k)(n-k-1)}{n(n-1)} \left( \frac{n(n+1)(3n^2-n-2)}{12} \right)
  - (n-k)^2 \frac{(n+1)^2}4 \\
  &= \boxed{\frac{k(n+1)(n-k)}{12}}.
\end{align*}

\clearpage
\section{}
First consider the special case of $m=n$.
Clearly,
we must have
\begin{align*}
  \E[S_m / S_n]
  &= 1.
\end{align*}
Note we use the assumption here that $\E[X_1^{-1}]$ exists.

By the linearity of expectation,
\begin{align*}
  \E[S_m / S_n]
  &= \sum_{i=1}^m \E[X_i/S_n].
\end{align*}
However,
since each $X_i$ is iid,
we conclude that
\[
  \E[X_i/S_n] = \frac1n
\]
so that
\[
  \E[S_m / S_n] = \frac{m}{n}
\]
as desired.

\clearpage
\section{}
We remark that
\begin{align*}
  \P\set{N>n}
  &= \P\set*{\sum_{i=1}^n X_i \leq x} \\
  &=: p_{n, x}.
\end{align*}

We argue by induction that $p_{n, x} = \frac{x^n}{n!}$.
It is clear that $p_{1, x} = x$.
Now suppose the induction hypothesis holds up to $n-1$.
We have
\begin{align*}
  p_n
  &= \int_0^x \P\set*{\sum_{i=1}^{n-1} X_i\leq x-z} p(z) dz &&\text{$p(z)$ density of $U[0, 1]$} \\
  &= \int_0^x p_{n, x-z} dz \\
  &= \int_0^x \frac{(x-z)^{n-1}}{(n-1)!} \\
  &= \left[ -\frac{(x-z)^n}{n!} \right]_0^x \\
  &= \frac{x^n}{n!}.
\end{align*}

By induction,
we conclude the proof.

In order to compute the expectation,
recall the identity
\begin{align*}
  \E[N]
  &= \sum_{n=1}^\infty \P\set{N = n}\cdot n \\
  &= \sum_{n=1}^\infty \P\set{N\geq n} \\
  &= \sum_{n=0}^\infty \P\set{N > n} \\
  &= \sum_{n=0}^\infty \frac{x^n}{n!} \\
  &= \boxed{e^x}.
\end{align*}
We can similarly compute the variance as
\begin{align*}
  \E[N^2]
  &= \sum_{n=1}^\infty \P\set{N = n}\cdot n^2 \\
  &= \sum_{n=1}^\infty \P\set{N\geq n} [n^2 - (n-1)^2] \\
  &= \sum_{n=0}^\infty \P\set{N > n} [2n+1] \\
  &= 2 \sum_{n=1}^\infty \P\set{N > n}\cdot n + \sum_{n=0}^\infty \P\set{N>n} \\
  &= 2x \sum_{n=0}^\infty \frac{x^n}{n!} + e^x \\
  &= e^x(2x+1).
\end{align*}
It follows that the variance is
\begin{align*}
  \Var[N]
  &= \boxed{e^x(2x+1) - e^{2x}}.
\end{align*}

\clearpage
\section{}
let us determine the density of $XY, Z^2$ separately
and recall that the joint density is just the product of densities
since $XY, Z^2$ are independent.

Let $F_{\xi}, p_{\xi}$ denote the distribution function
and density of the random variable $\xi$,
respectively.

We have
\begin{align*}
  F_{XY}(t)
  &= \P\set{X\leq t} + \int_t^1 \P\set{Y\leq t/x} p_X(x) dx \\
  &= t + \int_t^1 \frac{t}{x} dx \\
  &= t + \left[ t\log x \right]_t^1 \\
  &= t - t\log t. &&t\in [0, 1]
\end{align*}
It follows that
\begin{align*}
  p_{XY}(t)
  &= \frac{d}{dt} F_{XY}(t) \\
  &= 1 - \log t - 1 \\
  &= -\log t. &&t\in [0, 1]
\end{align*}
Similarly,
\begin{align*}
  F_{Z^2}(t) &= \sqrt t &&t\in [0, 1] \\
  p_{Z^2}(t) &= \frac1{2\sqrt t}. &&t\in (0, 1]
\end{align*}

The joint density for $t_1, t_2\in [0, 1], t_2\neq 0$ is thus
\[
  \boxed{p_{XY, Z^2}(t_1, t_2) = - \frac{\log t_1}{2\sqrt{t_2}}}.
\]

We now compute the desired probability
\begin{align*}
  \P\set{XY < Z^2}
  &= \int_0^1 \P\set{XY\leq t} p_{Z^2} (t) dt \\
  &= \int_0^1 (t- t\log t) \frac1{2\sqrt t} dt \\
  &= \frac12 \int_0^1 \sqrt t - \sqrt t \log t dt \\
  &= \left[ - \frac{t^{3/2} (3\log x - 5)}9 \right]_0^1 \\
  &= \frac59
\end{align*}
as desired.

\clearpage
\section{}
\subsection{}
Write $N$ to be the random variable indicating the number of draws until we stop.
Clearly for $n\leq 3$,
\[
  \P\set{N=n} = 0.
\]

Let us compute the distribution function $F(n) := \P\set{N\leq n}$.
This the probability that after $n$ draws,
we have at least one card from each shade.
This is equal to 1 minus the probability that we see at most 3 shades.
Let $p_1, p_2, p_3$ denote the probability we do not see 1, 2, 3 shades for some FIXED shades
(it is symmetric so it does not matter the particular choice of shades),
respectively.
By the inclusion-exclusion principle,
the desired probability is thus
\begin{align*}
  F(n)
  &= 1 - \binom41 p_1 + \binom42 p_2 - \binom43 p_3 \\
  &= 1- 4 \left( \frac34 \right)^n + 6 \left( \frac24 \right)^n - 4 \left( \frac14 \right)^n. &&n\geq 4
\end{align*}
This is because we need to count for the $\binom41$ ways we avoid a particular shade,
but avoid double counting the $\binom42$ ways we avoid a particular pair of shades,
and adjust for the overcorrection of the $\binom43$ ways we avoid any triple of shades.

Then for $n\geq 4$,
\begin{align*}
  \P\set{N=n}
  &= F(n) - F(n-1) \\
  &= - 4 \left( \frac34 \right)^n + 6 \left( \frac24 \right)^n - 4 \left( \frac14 \right)^n
  + 4 \left( \frac34 \right)^{n-1} - 6 \left( \frac24 \right)^{n-1} + 4 \left( \frac14 \right)^{n-1} \\
  &= \boxed{\left( \frac34 \right)^{n-1} - 3 \left( \frac24 \right)^{n-1} + 3 \left( \frac14 \right)^{n-1}}.
\end{align*}

\subsection{}
It is easy to see the that the sum of probabilities tend to 1
since we computed the individual probabilities as the difference between consecutive values of the distribution function.
Thus the partial sum up to $n$ is just $F(n)$
and $F(n)\to 1$ as $n\to \infty$ by inspection.

\end{document}
